{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data-preprocessing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iObkS2xpsTvT"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CTrYibHsSs6"
      },
      "source": [
        "# Setup TensorBoard for monitoring\n",
        "# https://colab.research.google.com/github/tensorflow/tensorboard/blob/master/docs/tensorboard_in_notebooks.ipynb#scrollTo=hzm9DNVILxJe\n",
        "\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEqGdkM9sLBx",
        "outputId": "ac9611dd-8f6f-4baf-d089-847dc553bcca"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import glob\n",
        "from pprint import pprint\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "drive.mount('/content/drive') \n",
        "\n",
        "BASE_PATH = '/content/drive/Shareddrives/Geospatial Hackathon 2021/hackathon'\n",
        "DATA_PATH = BASE_PATH + '/data'\n",
        "\n",
        "# Paths to input data\n",
        "IMAGE_PATH = DATA_PATH + '/20210417144750'\n",
        "IMAGE_GLOB_PATH = IMAGE_PATH + '/*.tif'\n",
        "METADATA_PATH = DATA_PATH + '/output3.csv'\n",
        "\n",
        "# Paths for output data\n",
        "TENSORBOARD_PATH = BASE_PATH + '/logs'\n",
        "MODEL_PATH = BASE_PATH + '/models'\n",
        "\n",
        "# Max size of the training dataset to use (None to use all available data)\n",
        "TRAINING_DATA_SIZE = 2000\n",
        "\n",
        "# Size of the images being used\n",
        "TARGET_SIZE = (500, 500)\n",
        "\n",
        "# Batch size for training\n",
        "BATCH_SIZE = 10\n",
        "\n",
        "pprint(os.listdir(BASE_PATH))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "['data',\n",
            " 'tensorflow',\n",
            " 'Report draft.gdoc',\n",
            " 'datamapping.csv',\n",
            " 'models',\n",
            " 'logs',\n",
            " 'report template.gdoc',\n",
            " 'modelsstd_scaler.bin',\n",
            " 'std_scaler.bin']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nR0PlE91scTi"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Y_JMGoNsbTn",
        "outputId": "9ccc807e-8bb0-47ba-a50c-aa3f047b5f67"
      },
      "source": [
        "dataset = pd.read_csv(DATA_PATH + '/output3.csv')\n",
        "dataset.head\n",
        "import sklearn\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.externals.joblib import dump, load\n",
        "\n",
        "# Target\n",
        "# image, crash_severity, holiday, light, weather, speed_limit\n",
        "crash_data = dataset.iloc[:,[0,5,6,7]]\n",
        "y = dataset.iloc[:,[3]]\n",
        "\n",
        "# One hot encode categorical features\n",
        "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [2])], remainder='passthrough')\n",
        "crash_data = ct.fit_transform(crash_data)\n",
        "print(f'\\n----- Encoded Crash Data\\n{crash_data}')\n",
        "\n",
        "# Get encoding <-> category mapping\n",
        "encoder = ct.named_transformers_['encoder']\n",
        "categories = encoder.categories_[-1].reshape(-1, 1)\n",
        "encoding = encoder.transform(categories).toarray()\n",
        "mapping = {elem[-1]: elem[:-1].astype('float32') for elem in np.concatenate([encoding, categories], 1)}\n",
        "print(f'\\n----- Mapping')\n",
        "pprint(mapping)\n",
        "\n",
        "# Spliting into train and test set\n",
        "print(f'\\n----- data spliting')\n",
        "X_train, X_test, y_train, y_test = train_test_split(crash_data,y, test_size = 0.2, random_state=1)\n",
        "print(f'X_train shape = {X_train.shape}')\n",
        "print(f'y_train shape = {y_train.shape}')\n",
        "print(f'sample row: {X_train[0]}')\n",
        "print(f'X_test shape = {X_test.shape}')\n",
        "print(f'y_test shape = {y_test.shape}')\n",
        "print(f'sample row: {X_test[0]}')\n",
        "\n",
        "\n",
        "sc = StandardScaler()\n",
        "print(f'\\n----- Standard scaling')\n",
        "temp = (pd.DataFrame(X_train))\n",
        "temp = temp.iloc[:,[0,1,2,4,5]].values\n",
        "sc.fit_transform(temp)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----- Encoded Crash Data\n",
            "[[1.0 0.0 0.0 '-36.853705673215856_174.72299370094953' 1.0 50]\n",
            " [1.0 0.0 0.0 '-36.853705673215856_174.72299370094953' 0.0 50]\n",
            " [1.0 0.0 0.0 '-36.853705673215856_174.72299370094953' 0.0 50]\n",
            " ...\n",
            " [0.0 0.0 1.0 '-36.9559938077771_174.8661960210974' 0.66 50]\n",
            " [0.0 0.0 1.0 '-36.9559938077771_174.8661960210974' 0.66 60]\n",
            " [0.0 0.0 1.0 '-36.9559938077771_174.8661960210974' 0.0 50]]\n",
            "\n",
            "----- Mapping\n",
            "{'F': array([1., 0., 0.], dtype=float32),\n",
            " 'HR': array([0., 1., 0.], dtype=float32),\n",
            " 'LR': array([0., 0., 1.], dtype=float32)}\n",
            "\n",
            "----- data spliting\n",
            "X_train shape = (62608, 6)\n",
            "y_train shape = (62608, 1)\n",
            "sample row: [0.0 0.0 1.0 '-36.8741633001281_174.75572565984046' 0.66 50]\n",
            "X_test shape = (15653, 6)\n",
            "y_test shape = (15653, 1)\n",
            "sample row: [1.0 0.0 0.0 '-36.861888723980755_174.7639086495632' 0.0 100]\n",
            "\n",
            "----- Standard scaling\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.99468479, -0.20244913,  2.27874538,  0.13034556, -0.51862695],\n",
              "       [ 0.50133234, -0.20244913, -0.43883797,  0.9714686 , -0.51862695],\n",
              "       [-1.99468479,  4.93951242, -0.43883797, -1.5024227 , -0.51862695],\n",
              "       ...,\n",
              "       [ 0.50133234, -0.20244913, -0.43883797,  0.9714686 , -0.51862695],\n",
              "       [ 0.50133234, -0.20244913, -0.43883797, -0.68603857, -0.51862695],\n",
              "       [-1.99468479, -0.20244913,  2.27874538,  0.13034556,  2.01535537]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNTr6c8LNbAB"
      },
      "source": [
        "# Saving everything"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnaRem2LNacC",
        "outputId": "56bae4a1-8ac7-4f03-b610-c6c234ec80a7"
      },
      "source": [
        "pd.DataFrame(X_train).to_csv(DATA_PATH + \"/train/X_train.csv\", index=False)\n",
        "pd.DataFrame(y_train).to_csv(DATA_PATH + \"/train/y_train.csv\", index=False)\n",
        "pd.DataFrame(X_test).to_csv(DATA_PATH + \"/test/X_test.csv\", index=False)\n",
        "pd.DataFrame(y_test).to_csv(DATA_PATH + \"/test/y_test.csv\", index=False)\n",
        "pd.DataFrame(mapping).to_csv(DATA_PATH + \"mapping.csv\", index=False)\n",
        "dump(sc, BASE_PATH + '/std_scaler.bin', compress=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/Shareddrives/Geospatial Hackathon 2021/hackathon/std_scaler.bin']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    }
  ]
}